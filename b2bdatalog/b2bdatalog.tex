%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
%%%% Small single column format, used for CIE, CSUR, DTRAP, JACM, JDIQ, JEA, JERIC, JETC, PACMCGIT, TAAS, TACCESS, TACO, TALG, TALLIP (formerly TALIP), TCPS, TDSCI, TEAC, TECS, TELO, THRI, TIIS, TIOT, TISSEC, TIST, TKDD, TMIS, TOCE, TOCHI, TOCL, TOCS, TOCT, TODAES, TODS, TOIS, TOIT, TOMACS, TOMM (formerly TOMCCAP), TOMPECS, TOMS, TOPC, TOPLAS, TOPS, TOS, TOSEM, TOSN, TQC, TRETS, TSAS, TSC, TSLP, TWEB.
% \documentclass[acmsmall]{acmart}

%%%% Large single column format, used for IMWUT, JOCCH, PACMPL, POMACS, TAP, PACMHCI
% \documentclass[acmlarge,screen]{acmart}

%%%% Large double column format, used for TOG
% \documentclass[acmtog, authorversion]{acmart}

%%%% Generic manuscript mode, required for submission
%%%% and peer review
\documentclass[manuscript,screen,review]{acmart}
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
\providecommand\BibTeX{{%
\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
	conference title from your rights confirmation emai}{June 03--05,
	2018}{Woodstock, NY}
%
%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!
%
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
	June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%% \citestyle{acmauthoryear}
\usepackage{microtype}  %towards typographic perfection...
\usepackage{tikz}
\usepackage{tikz-qtree}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows.meta, shapes.misc, positioning}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\usepackage[ruled, vlined, linesnumbered]{algorithm2e}
\SetKw{True}{true}
\SetKw{False}{false}
\SetKwData{typeInt}{Int}
\SetKwData{typeRat}{Rat}
\SetKwData{Defined}{Defined}
\SetKwFunction{parseStatement}{parseStatement}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{A Back-to-basics Empirical Study of Datalog Materialization}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Bruno Rucy Carneiro Alves de Lima}
\authornote{Both authors contributed equally to this research.}
\email{bruno.rucy.carneiro.alves.de.lima@ut.ee}
\orcid{1234-5678-9012}

\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
	\institution{Institute for Clarity in Documentation}
	\streetaddress{P.O. Box 1212}
	\city{Dublin}
	\state{Ohio}
	\country{USA}
	\postcode{43017-6221}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
\institution{The Th{\o}rv{\"a}ld Group}
\streetaddress{1 Th{\o}rv{\"a}ld Circle}
\city{Hekla}
\country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
	\institution{Inria Paris-Rocquencourt}
	\city{Rocquencourt}
	\country{France}
}

\author{Aparna Patel}
\affiliation{%
	\institution{Rajiv Gandhi University}
	\streetaddress{Rono-Hills}
	\city{Doimukh}
	\state{Arunachal Pradesh}
	\country{India}}

\author{Huifen Chan}
\affiliation{%
	\institution{Tsinghua University}
	\streetaddress{30 Shuangqing Rd}
	\city{Haidian Qu}
	\state{Beijing Shi}
	\country{China}}

\author{Charles Palmer}
\affiliation{%
	\institution{Palmer Research Laboratories}
	\streetaddress{8600 Datapoint Drive}
	\city{San Antonio}
	\state{Texas}
	\country{USA}
	\postcode{78229}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
\institution{The Th{\o}rv{\"a}ld Group}
\streetaddress{1 Th{\o}rv{\"a}ld Circle}
\city{Hekla}
\country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
	\institution{The Kumquat Consortium}
	\city{New York}
	\country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato and Tobin, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
	A clear and well-documented \LaTeX\ document is presented as an
	article formatted for publication by ACM in a conference proceedings
	or journal publication. Based on the ``acmart'' document class, this
	article presents and explains many of the common variations, as well
	as many of the formatting elements an author may use in the
	preparation of the documentation of their work.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010520.10010553.10010562</concept_id>
	<concept_desc>Computer systems organization~Embedded systems</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10010520.10010575.10010755</concept_id>
	<concept_desc>Computer systems organization~Redundancy</concept_desc>
	<concept_significance>300</concept_significance>
	</concept>
	<concept>
	<concept_id>10010520.10010553.10010554</concept_id>
	<concept_desc>Computer systems organization~Robotics</concept_desc>
	<concept_significance>100</concept_significance>
	</concept>
	<concept>
	<concept_id>10003033.10003083.10003095</concept_id>
	<concept_desc>Networks~Network reliability</concept_desc>
	<concept_significance>100</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
	\includegraphics[width=\textwidth]{sampleteaser}
	\caption{Seattle Mariners at Spring Training, 2010.}
	\Description{Enjoying the baseball game from the third-base
		seats. Ichiro Suzuki preparing to bat.}
	\label{fig:teaser}
\end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\textbf{Motivation.} SQL\cite{key} has been the \textit{de facto} universal relational
database interface for querying and management since its inception,
with all other alternatives having had experienced disproportionately
little interest. The reasons for this are many, out of which only one
is relevant to this narrative; performance.

The runner-ups of popularity were languages used for machine reasoning,
a subset of the Artificial Intelligence field that attempts to attain
intelligence through the usage of rules over knowledge. The canonical
language for reasoning over relational databases is datalog\cite{datalog}.
Similarly to SQL, it also is declarative, however, its main semantics' difference
is in the support for recursion while still ensuring termination irrespective
of the program being run.

A notable issue with respect to real-world adoption of datalog is in
performance. The combined complexity of evaluating a program is
$\text{EXPTIME}$\cite{datalog}, while SQL queries are $\text{AC}^0$. It was
not until recently\cite{rdfox} that scalable implementations were
developed.

Digital analytics has been one of the main drivers of the recent datalog renaissance,
with virtually all big-data oriented datalog implementations having had either been built
on top of the most mainstream industry oriented frameworks\cite{bigdatalog,cog,cog2} or with the
aid of the most high-profile technology companies\cite{logica,yedalog,vadalog}.

Another strong source of research interest has been from the knowledge graph community. A
knowledge graph \textit{KG} is a regular relational database \textit{I} that contains
\textit{ground truths}, and rules.

The most important datalog operation is called \textit{materialization}, the derivation of all
truths that follow from the application of rules over the relational database, with the most
straightforward goal being to ensure queries do not need any reasoning.

\textbf{Problem.} The maintenance of materialization is possibly the most important issue
that plagues the real-world usage of datalog, and has been a source of much research interest,
most commonly under the guise of incremental view maintenance.

The handling of additions is efficiently done with the semi-naive evaluation method\cite{sne},
that can be notoriously efficient for a special class of programs that are popular in practice.
Deletions on the other hand, are possibly much less efficient, since the retraction of a \textit{ground truth}
naively implies the deletion of all data derived from it.

The quintessential algorithm for computing the materialization adjustment with respect to a
deletion is the delete-rederive\cite{dred} method, that generates new datalog programs to
compute the deletions necessary to ensure correctness, doing so by first computing all possible
deletions, and then all possible alternative derivations, with the adjusment being the difference
between both of these sets.

The fact that two different algorithms are used implies that there are two different performance
characteristics for additions and deletions, in spite of dred utilizing semi-naive evaluation as well,
therefore there being possibly-severe biases in performance. To this date, there hasn't been
a study that evaluated a datalog engine performance in properly dynamic scenarios.

A promising computation framework that could be well-suited to this problem is differential
dataflow\cite{differential_dataflow}. Timely dataflow is a high-throughput, low-latency
streaming-and-distributed computational model that extends the dataflow paradigm. The timely
in its name refers to it thoroughly establishing the semantics of each aspect of communication
notification, taking the parallelization blueprint and making its execution transparent to the user.

A notification sent between operators is assigned a timestamp, that needs only to be partially
ordered, with no assumptions being made with respect to insertion. The goal is to use these to
correctly reason about the data with respect to the local operator states' of computation completion, in
order to cleverly know with certainty when not only whether should work be done asynchronously
or with sharding, but whether there is data yet to be processed or not.

Differential Dataflow, a generalization of incremental processing, is an extension to the
Timely Dataflow model. While the shape of the data that flows in the latter is in the form
of \verb|(data, timestamp)|, the former's is \verb|(data, timestamp, multiplicity)|, therefore
restricting its bag-of-data semantics to multisets, with the multiplicity element representing a
difference, for instance, $+1$ representing the addition of $1$ datapoint arriving at a
specific timestamp, or $-1$, a deletion.

The key difference between both models is that similarly to how timely makes distribution
transparent, differential does the same with incremental computation, by providing to the user the
possibility of not only adding to a stream, the only possible action in timely dataflow, but
allow it to respond to arbitrary additions and retractions at any timestamp.  There is one
reasoner that uses differential dataflow\cite{ddlog}, however, it has no storage layer, instead
it compiles fixed datalog programs into fixed differential dataflow programs, that can only run in one computer.

The lack of a canonical open-source implementation of datalog makes attempts at making
empirical statements about performance-impacting theoretical developments, such as the usage of
differential dataflow, brittle and difficult, since there is no point of reference to
compare and validate, and comparisons against commercial implementations are not
reliable, since optimizations might be trade secrets.

A notorious exploration that highlights this issue is the COST, Configuration That Outperforms
a Single Thread, article\cite{COST}, in which the author posits that multiple published
graph-processing engines are likely never able to outperform simple single-threaded implementations.
Some high-profile datalog implementations were built upon systems mentioned on that article.
Later on, the author made multiple pieces of informal writing in which the most performant datalog
engines were investigated for COST\cite{blogdynamicdatalog, blogvldbsigmod}, with results that
showed a very different picture than the ones depicted by the original articles.

\textbf{Methodology.} To address the aforementioned problem, we conduct a back-to-basics
empirical study of datalog program materialization, revisiting and measuring the core assumptions that go
into the implementation of a reasoner. We coalesce this knowledge into a framework that provides multiple shared components for reasoning research. Straightforward single-threaded, parallel and distributed
implementations, with the last being done with differential dataflow, of both substitution-based
and relational-algebra interpretations are realized alongside common well-known optimizations.

\textbf{Contributions.} In this article we make several contributions to clarifying, benchmarking
and easing pushing the boundaries of datalog evaluation engineering research further by
providing performant and open-source implementations of single-threaded, parallel and distributed evaluators.

\begin{itemize}
	\item \textbf{Techniques and Guidelines.} We study the challenge of building a reasoner from scratch,
	      with no underlying framework, and ponder over all decisions necessary in order to bring that to fruition,
	      alongside with relevant recent literature.
	\item \textbf{Differential Dataflow.} We accurately investigate the suitability of differential
	      dataflow for datalog, and showcase how does it fare against the ubiquitous DRED and semi-naive
	      evaluation based reasoners with the same language, base and with the same data format.
	\item \textbf{Framework.} All code outputs of this article are coalesced in a rust library named shapiro,
	      consisted of a datalog to relational algebra rewriter, a relational algebra evaluator, and two datalog engines,
	      one that is parallel-capable and supports both substitution-based and relational algebra methods, and other that
	      relies on the state-of-the-art differential dataflow\cite{differential_dataflow} distribution computation framework.
	      The main expected outcome of this library is to provide well-understood-and-reasoned-about baseline implementations
	      from where future research can take advantage of, and reliable COST configurations can be attained.
	\item \textbf{Benchmarking.} We perform two thorough benchmark suites. One evaluates the performance of the developed
	      relational reasoner with multiple different index data structures, and another that compares the performance
	      of the distributed reasoner against four state-of-the-art distributed reasoners. The selected datasets are either
	      from the program analysis, heavy users of not-distributed datalog, or from the semantic web community
	      , which has multiple popular infinitely-scalable benchmarks, and are the main proponents of existential datalog.
\end{itemize}

\section{Related Work}

\textbf{Datalog engines.} There are two kinds of recent relevant datalog engines. The first encompasses
those that push the performance boundary, with the biggest proponents being RDFox\cite{rdfox}, that
proposes the to-date, according to their benchmarks, most scalable parallelisation routine, RecStep\cite{recstep},
that builds on top of a highly efficient relational engine, and DCDatalog\cite{dcdatalog}, that builds upon
an influential query optimizer, DeALS\cite{deals} and extends a work that establishes how some linear
datalog programs could be evaluated in a lock-free manner, to general positive programs.

One of the most high-profile datalog papers of interest has been BigDatalog\cite{bigdatalog}, that
originally used the query optimizer DeALs, and was built on top of the very popular Spark\cite{spark}
distribution framework. Soon after, a prototypical implementation\cite{cog} over Flink\cite{flink},
a distribution framework that supports streaming, Cog, followed. Flink, unlike Spark, supports
iteration, so implementing reasoning did not need to extend the core of the underlying framework. The most
successful attempt at creating a distributed implemention has been Nexus\cite{nexus}, that is also built on Flink,
and makes use of its most advanced feature, incremental stream processing. To date, it is the fastest distributed
implementation.

\textbf{Data structures used in datalog engines.} The core of each datalog engine is consisted of possibly
two main data structures: one to hold the data itself, and another for indexes. Surprisingly little regard is given to
this, compared to algorithms themselves, despite potentially being one of the most detrimental factors for performance
. Binary-decision diagrams\cite{bddbddb}, hash sets\cite{microzee} and B-Trees\cite{souffle_btree} are often used as
either one or both main structures. An important highlight of the importance of data structure implementation is how
in \cite{souffle_btree} Subotic et al, managed to attain an almost 50 times higher performance in certain benchmarks
than other implementations of the same data structure.

\section{Datalog Evaluation}

In this section we review the basics of all concepts related to datalog evaluation, as it is done in the current time.

\subsection{Datalog}

\textit{Datalog}\cite{all_you_ever_wanted_to_ask} is a declarative programming language. A program $P$ is a set of
rules $r$, with each $r$ being a restricted first-order formula of the following form: \[\bigwedge_{i=1}^kB_i(x_1, ..., x_j) \rightarrow \exists (y_1, ..., y_j)H(x_1, ..., x_j, y_1, ..., y_j)\]
with $k$, $j$ as finite integers, $x$ and $y$ as terms, and each $B_i$ and $H$ as predicates. A term can belong
either to the set of variables, or constants, however, it is to be noted that all $y$ are existentially quantified.
The set of all $B_i$ is called the \textit{body}, and $H$ the \textit{head}.

A rule $r$ is said to be datalog, if the set of all $y$ is empty, and no predicate is negated, conversely, a
datalog program is one in which all rules are datalog.
\begin{exmp}{Datalog Program}\label{ex1}
	\[
		P = \left\{  \begin{array}{l}
			\text{SubClassOf}(?x, ?y) \wedge \text{SubClassOf}(?y, ?z) \rightarrow \text{SubClassOf}(?x, ?z) \\
		\end{array}\right\}
	\]
\end{exmp}
Example \ref{ex1} shows a simple valid recursive program. The only rule denotes that \textit{for all x, y, z, if x is
	in a SubClassOf relation with y, and y is in a SubClassOf relation with z, then it follows that x is in a subClassOf
	relation with z}.

The meaning of a datalog program is often\cite{datalog} defined through a \textit{Herbrand Interpretation}. The first step
to attain it is the \textit{Herbrand Universe} $\mathfrak{U}$, the set of all constant, commonly referred to as \textit{ground}, terms.
\begin{exmp}{Herbrand Universe}\label{ex2}
	\[
		S = \left\{ \begin{array}{l}
			\text{SubClassOf}(\text{professor}, \text{employee}) \\
			\text{SubClassOf}(\text{employee}, \text{taxPayer})  \\
			\text{SubClassOf}(\text{employee}, \text{employed})  \\
			\text{SubClassOf}(\text{employed}, \text{employee})
		\end{array}\right\}
	\]
	\[
		\mathfrak{U} = \left\{  \begin{array}{l}
			\text{professor}, \text{employee}, \text{employed}, \text{taxPayer}
		\end{array}\right\}
	\]
\end{exmp}
From the shown universe on example \ref{ex2}, it is possible to build The \textit{Herbrand Base}, the set of all possible truths,
from \textit{facts}, assertions that are true, as represented by the actual constituents of the SubClassOf set.
\begin{exmp}{Herbrand Base}\label{ex3}
	\[
		\mathfrak{B} = S \cup \left\{  \begin{array}{l}
			\text{SubClassOf}(\text{professor}, {professor}) \\
			\text{SubClassOf}(\text{employee}, {employee})   \\
			\text{SubClassOf}(\text{employed}, {employed})   \\
			\text{SubClassOf}(\text{taxPayer}, {taxPayer})   \\
			\text{SubClassOf}(\text{professor}, {taxPayer})  \\
			\text{SubClassOf}(\text{taxPayer}, {professor})  \\
			\text{SubClassOf}(\text{employee}, {professor})  \\
			\text{SubClassOf}(\text{taxPayer}, {employee})
		\end{array}\right\}
	\]
\end{exmp}
On example \ref{ex3}, all facts are indeed \textit{possible}, but not necessarily \textit{derivable} from the actual data and
program. An interpretation $I$ is a subset of $\mathfrak{B}$, and a \textit{model} is an interpretation such that all rules
are satisfied. A rule is satisfied if either the head is true, or if the body is not true.
\begin{exmp}{Models}\label{ex4}
	\[
		I_1 = S \cup \left\{  \begin{array}{l}
			\text{SubClassOf}(\text{professor}, {taxPayer}) \\
			\text{SubClassOf}(\text{employee}, {employee})
		\end{array}\right\}
	\]
	\[
		I_2 = S \cup \left\{  \begin{array}{l}
			\text{SubClassOf}(\text{professor}, {taxPayer}) \\
			\text{SubClassOf}(\text{employee}, {employee})  \\
			\text{SubClassOf}(\text{employed}, {employed})
		\end{array}\right\}
	\]
	\[
		I_3 = S \cup \left\{  \begin{array}{l}
			\text{SubClassOf}(\text{professor}, {taxPayer}) \\
			\text{SubClassOf}(\text{employee}, {employee} ) \\
			\text{SubClassOf}(\text{employed}, {employed})  \\
			\text{SubClassOf}(\text{professor, professor})
		\end{array}\right\}
	\]
\end{exmp}
The first interpretation, $I_1$, from example \ref{ex4}, is not a model, since $\text{SubClassOf}(\text{employed}, \text{employed})$ is satisfied
and present. Despite both $I_2$ and $I_3$ being models, $I_2$ is the \textit{minimal} model, which is the definition of the meaning of the program
over the data. The input data, the database, is named as the \textit{Extensional Database} $EDB$, and the output of the program is the \textit{Intensional Database}
$IDB$.

Let an $DB = EDB \cup IDB$, and for there to be a program $P$. We define the \textit{immediate consequence} of $P$ over $DB$ as all facts
that are either in $DB$, or stem from the result of applying the rules in $P$ to $DB$. The \textit{immediate consequence operator}
$\textbf{I}_C(DB)$ is the union of $DB$ and its immediate consequence, and the $IDB$, at the moment of the application of $\textbf{I}_C(DB)$
is the difference of the union of all previous $DB$ with the $EDB$.


It is trivial to see that $I_C(DB)$ is monotone, and given that both the $EDB$ and $P$ are finite sets, and that $IDB = \emptyset$ at the start,
at some point $I_C(DB) = DB$, since there won't be new facts to be inferred. This point is the \textit{least fixed point} of $I_c(DB)$\cite{datalog},
and happens to be the \textit{minimal} model.
\begin{exmp}{Repeated application of $I_c$}\label{ex5}
	\begin{align*}
		P = \{ Edge(?x, ?y) \rightarrow TC(?x, ?y), TC(?x, ?y), TC(?y, ?z) \rightarrow TC(?x, ?z) \} \\
		EDB = \{ Edge(1, 2), Edge(2, 3), Edge(3, 4) \}                                               \\
		DB = EDB                                                                                     \\
		DB = I_C(DB)                                                                                 \\
		DB == EDB \cup \{ TC(1, 2), TC(2, 3), TC(3, 4) \}                                            \\
		DB = I_C(DB)                                                                                 \\
		DB == EDB \cup \{ TC(1, 2), TC(2, 3), TC(3, 4), TC(1, 3), TC(2, 4) \}                        \\
		DB = I_C(DB)                                                                                 \\
		DB == EDB \cup \{ TC(1, 2), TC(2, 3), TC(3, 4), TC(1, 3), TC(2, 4), TC(1, 4) \}              \\
		DB = I_C(DB)                                                                                 \\
		DB == EDB \cup \{ TC(1, 2), TC(2, 3), TC(3, 4), TC(1, 3), TC(2, 4), TC(1, 4) \}              \\
		IDB = DB \setminus EDB
	\end{align*}
\end{exmp}
The introduced form of evaluation, with a walkthrough given on example \ref{ex5}, is called \textit{naive}, meanwhile, the ubiquitous evaluation
mechanism, as of the date of writing this paper, is the \textit{semi-naive} one. The only difference is that \textit{semi-naive} does not
repeatedly union the $EDB$ with the entire $IDB$, but does so only with the difference of the previous immediate consequence with the $IDB$. This can
be hinted from the example, where each next application of $I_c$ only renders new facts from the previous newly derived ones.

\subsection{Infer}
The most relevant performance-oriented aspect of both of the introduced evaluation mechanisms is the implementation of $I_c$ itself. The two
most high-profile methods to do so are either purely evaluating the rules, or rewriting them in some other imperative formalism, and executing it.

The Infer\cite{datalog} algorithm is the simplest example of the former, and relies on substitutions. A substitution $S$ is a homomorphism
$[x_1 \rightarrow y_1, ..., x_i \rightarrow y_i]$, such that $x_i$ is a variable, and $y_i$ is a constant. Given a not-ground fact,
such as $TC(?x, 4)$, \textit{applying} the substitution $[?x \rightarrow 1]$ to it will yield the ground fact $TC(1, 4)$.

Infer relies on attempting to build and extend substitutions for each fact in each rule body over every single $DB$ fact. Once
all substitutions are made, they are applied to the heads of each rule. Every result of this application that is ground belongs
to the immediate consequence.

\subsection{Relational Algebra}
Relational Algebra\cite{codd_1970} is an imperative language, that explicitly denotes operations over sets of tuples with fixed arity, relations. It is
the most popular database formalism that there is, with virtually every single major database system adhering to the relational model\cite{pg,mysql,sqlserver}
and supporting relational algebra as the SQL compilation target.

Let $R$ and $T$ be relations with arity $r$ and $t$, $\theta$ be a binary operation with a boolean output, $R(i)$ be the i-th column in $R$, and
$R[h, ..., k]$ be the subset of $R$ such that only the columns $h, ..., k$ remain, and Const the set of all constant terms. The following
are the most relevant relational algebra operators and their semantics:
\begin{itemize}
	\item Selection by column $\sigma_{i=j)}(R) = \{ a \in R | a(i) == a(j) \}$
	\item Selection by value $\sigma_{i=k}(R) = \{a \in R | a(i) == k \}$
	\item Projection $\pi_{h, ..., k}(R) = \{(R(i), ..., R(j), \overrightarrow{C}) |  i, j >= 1 \wedge i, j <= r\ \wedge \forall c \in C. c \in \text{Const}$
	\item Product $\times(R, T) = \{(a, b) | a \in R \wedge b \in T \}$
	\item Join $\Join_{i=j} = \{(a, b) | a \in R \wedge b \in T \wedge a(i) == b(j)\}$
\end{itemize}

Rewriting datalog into some form of relational algebra has been the most successful strategy employed by the vast majority of all current state-of-the-art
reasoners\cite{bigdatalog, cog, nexus, recstep, dcdatalog, souffle} mostly due to the extensive industrial and academic research into developing data processing frameworks that
process very large amounts of data, and the techniques that have arisen from these.

In spite of this, there is no open-source library that provides a stand-alone datalog to relational algebra translator, therefore every single
datalog evaluator has to repeat this effort. Moreover, datalog rules translate to a specific form of relational algebra expressions, the
select-project-join $\mathcal{SPJ}$ form.

A relational algebra expression is in the $\mathcal{SPJ}$ form if it consists solely of select, project and join operators. This form
is very often seen in practice, being equivalent to \verb|SELECT ... FROM ... WHERE ...| SQL queries, and highly benefits from being
equivalent to conjunctive queries, that are equivalent to single-rule and non-recursive datalog programs.

We propose a straightforward not-recursive pseudocode algorithm to translate a datalog rule into a $SPJ$ expression tree, in which
relational operators are nodes and leaves are relations. The value proposition of the algorithm is for the resulting tree to be ready
to be recursively executed, alongside having two essential relational optimizations, selection by value pushdown, and melding selection
by column with products into joins, the most important relational operator.
\begin{algorithm} [!h]
	\scriptsize
	\KwIn{A datalog rule $\mathcal{R}$}
	\KwResult{A relational algebra expression $\mathcal{R}_a$}
	\SetKwData{s}{s}
	\BlankLine
	\SetKwProg{Fn}{Function}{ is}{end}
	\SetKwProg{For}{For}{:}{end}

	\Fn{toIncompleteExpression($r$ : Datalog Rule)}{
		let $t$ be a fresh tree

		\For{For every fact $a_i$ in the rule body $b$} {
			create a relation node $r_i$ with the same arity as $a_i$, and its terms representing columns. variable terms
			are column identifiers, and constant terms are temporary-lived proxies for selections.
			\If {$i < len(b) - 1$ } {
				add a product node $p_i$ to $t$.
				set $r_i$ as the left child of $p_i$.
			}\Else {
				\If {$len(b) > 1$} {
					set $r_i$ as the right child of $p_{i - 1}$
				}
			}
			\If {$i > 0$} {
				set $p_i$ as the right child of $p_{i - 1}$
			}
		}
		\textbf{return} $t$
	}
	\Fn{constantToSelection($e$ : Expression)}{
		let $t$ be a copy of $e$

		let $C$ be a map $C : \text{Const} \rightarrow \text{Var}$

		\For {every relation $r_i$ in $t$} {
			\For {every constant $c_j$ in $r_i$}{
				add a selection by value node $s_j$ to $t$ with column index $j$ and value $c_j$

				set $s_j$'s parent to $r_i$'s, and $r_i$ as its left child

				\If {$\neg (c_j \in C)$}{
					create a fresh variable term $v_j$ and store it in $C$ with $c_j$ as the key}
				\Else{replace $c_j$ for the value in $C$ under $c_j$}
			}
		}
		\textbf{return} $t$
	}
	\Fn{equalityToSelection($e$ : Expression)}{
		let $t$ be a copy of $e$

		let $V$ be a map $V : \text{Var} \rightarrow \mathbb{Z}$

		let $t_{p}$ be a pre-order traversal of $t$

		\For {every relation $r_i$ in $t_{p}$} {
			\For {every variable $v_j$ in $r_i$}{
				\If{$\neg (v_j \in V)$}{
					add $v_j$ to $V$ with $j$ as the value
				}\Else{let $k$ be the value of $v_j$ in $V$
					let $p_i$ be the first product to the left of $r_i$ in $t_{p}$

					add a selection by column node $s_j$ to $t$ with left column index $k$ and right column index $j$

					set $s_j$'s parent to $p_i$'s, and $p_i$ as its left child
				}
			}
		}
		\textbf{return} $t$
	}
	\Fn{projectHead($e$ : Expression, $r$ : Datalog Rule)}{
		let $n$ be 0

		let $t$ be a copy of $e$

		let $h$ be the head of $r$

		let $t_{p}$ be a pre-order traversal of $t$

		let $V$ be a map $V : \text{Var} \rightarrow \mathbb{Z}$

		\For {every relation $r_i$ in $t_{p}$}{
			\For {every term $x$ in $r_i$}{
				\If {$x$ is a variable} {
					add $x$ to $V$ with $n$ as value
				}\Else{
					continue
				}
				$n$ += 1

				add projection node $z$ to $t$ and set it as root with an empty list

				\For {every term $x$ in $h$}{
					\If {$x$ is a constant}{
						push $x$ into $z$
					}\Else {
						let $k$ be the value of $x$ in $V$

						push $k$ into $z$
					}
				}
			}
		}

		\textbf{return} $t$
	}
	\Fn{productToJoin($e$ : Expression)}{
		let $t$ be a copy of $e$

		let $t_{p}$ be a pre-order traversal of $t$

		\For {every selection by column $s_i$ in $t_{p}$}{
			find the first product $p_j$ after $s_i$ in $t_{p}$

			remove $s_i$ from $t$, swap $p_j$ for a join $g_j$ in $t$ with left and right column indexes by those of $s_i$
		}
		\textbf{return} t
	}
	$\mathcal{R}_a$ = toIncompleteExpression($\mathcal{R}$)

	$\mathcal{R}_a$ = constantToSelection(expression, $\mathcal{R}$)

	$\mathcal{R}_a$ = equalityToSelection(expression, $\mathcal{R}$)

	$\mathcal{R}_a$ = projectHead(expression, $\mathcal{R}$)

	$\mathcal{R}_a$ = productToJoin(expression, $\mathcal{R}$)

	\textbf{return} $\mathcal{R}_a$
	\caption{An algorithm to translate a datalog rule into relational algebra}
	\label{alg:datalog_to_relalg}
\end{algorithm}
The canonical algorithms for translating datalog to relational algebra\cite{opt_sys_alg_ev_dat, log_dab} are recursive, complex,
and do not assume the output to be a tree, instead being mostly symbolic.

\section{Shapiro}
In order to coalesce all contributions in this paper, an extensible reasoning system was designed, from scratch. No frameworks
were used, nor any code related to any other reasoner has been reutilized, furthermore, a modern, performant and safe programming
language was used, Rust\cite{rust_lang}.

The main rationale for this choice is twofold: garbage-collected languages such as java are hard to benchmark, and tuning
performance, alongside reasoning about it, often requires the developer to either learn highly specific minutiae relating
to the compiler or interpreter, and, or, the garbage collector.

All most performant shared-memory reasoners are unsurpisingly all implemented in C++\cite{rdfox, vadalog, souffle}, since
it provides manual memory management and is the \textit{de facto} language for high-performance computing. Rust is a
recent language, approximately 10 years old, that exhibits similar or faster performance than C++, with its \textit{raison d'etre}
being that, unlike C++, it is memory-safe and thread-safe\cite{rust2014,rustbelt,rustsafety}; both of these guarantees are
of incredible immediate benefit to writing a reasoner.

Shapiro, the developed system, is fully modular and offers a heap of independent components and interfaces.

\begin{figure} [htb!]
	\centering
	\begin{tikzpicture}
		\begin{scope}
			\node (traits) at (0, 0) [draw, dotted, rectangle, minimum width=5cm, minimum height=5cm] {};
			\node (traits_title) at (traits.north east) [draw, anchor = north east] {Traits};

			\node (ev_mec) at (traits.east) [draw, dotted, rectangle, minimum width=5cm, minimum height=5cm, anchor=west] {};
			\node (ev_mec_title) at (ev_mec.north east) [draw, anchor=north east] {Evaluation Mechanisms};

			\node (ds) at (traits.south) [draw, dotted, rectangle, minimum width=5cm, minimum height=5cm, anchor=north] {};
			\node (ds_title) at (ds.north east) [draw, anchor=north east] {Data Structures};

			\node (engines) at (ds.east) [draw, dotted, rectangle, minimum width=5cm, minimum height=5cm, anchor=west] {};
			\node (engines_title) at (engines.north east) [draw, anchor=north east] {Engines};
		\end{scope}

		% Traits
		\begin{scope}
			\node (insta) [draw, below = 3mm of traits.north] {RuleEvaluator};
			\node (bupe) [draw, below of = insta] {BottomUpEvaluator};
			\node (tode) at (traits) [draw, below of = bupe] {TopDownEvaluator};
			\node (idx) at (traits) [draw, below of = tode] {IndexBacking};
			\node (ty) at (traits.south) [draw, below of = idx] {Ty};
		\end{scope}

		% Data Structures

		\begin{scope}
			\node (btset) [draw, below = 10mm of ds.north] {BTreeSet};
			\node (hashm) at (ds) [draw, below of = btset] {HashMap};
			\node (vecto) at (ds) [draw, below of = hashm] {Vec};
			\node (spine) at (ds) [draw, below of = vecto] {Spine};
		\end{scope}

		% Evaluation Mechanisms

		\begin{scope}
			\node (relal) [draw, below = 10mm of ev_mec.north] {RelationalAlgebra};
			\node (subst) at (ds) [draw, below of = relal] {Substitution};
		\end{scope}

		% Engines

		\begin{scope}
			\node (chibi) [draw, below = 10mm of engines.north] {Chibi};
			\node (relational) at (engines) [draw, below of = chibi] {Relational};
			\node (differential) at (engines) [draw, below of = relational] {Differential};
		\end{scope}

	\end{tikzpicture}
	\caption{Shapiro Components}
	\label{fig:shapiro_comp}
\end{figure}
On figure \ref{fig:shapiro_comp} we can see the most relevant modules. The northwestern quadrant, Traits, refers to Rust Traits, constructs that
define \textit{behavior}, and that can be passed to functions. It is important to take note that Rust favors \textit{composition} over inheritance, unlike Java.
Nevertheless, all other quadrants are built upon these traits, and are therefore generic over any type that implements them.

A \verb|Rule Evaluator| represents $\textbf{T}_p$, requiring only that types implement a function \verb|evaluate|, that accepts an instance,
and produces another instance. The evaluation mechanisms \verb|Relational Algebra| and \verb|Substitution| both implement this trait,
evaluating datalog rules by either using the aforegiven translation algorithm, or using Infer. We also provide naive parallel evaluation of rules,
in which each rule, in both mechanisms, are sent to be executed in a threadpool. We leverage the highly efficient, and easy to use, requiring
only one additional line of code, rayon\cite{rayon} library. This should suffice as a transparent baseline from which other parallelization
strategies could be compared to.

The concept of fixpoint evaluation is a Struct, that consists of the ground fact database, $EDB$, a \verb|Rule Evaluator|, and an array of
instances. It is assumed that the datalog program is in the instance itself. Semi-naive evaluation is a method of fixpoint evaluation that takes
no arguments, and instead uses two additional instances, one to keep the current delta, and the previous one. This is succintly implemented in rust
in the same manner as this description.

\verb|BottomUpEvaluator| and \verb|TopDownEvaluator| both respectively denote the two kinds of evaluation, respectively, the explicit
materialization of the program, and the answering to a query with respect to a program. The former requires the implementation of a function that takes
in only a program as an argument, and the latter, a program and a goal.

The three engines, \verb|Chibi|, \verb|Relational| and \verb|Differential| all implement only \verb|BottomUpEvaluator|. In order to be able to
evaluate a program, it is required to have access to an instance. An instance is defined in code as it is in literature, a set of relations. Taking
inspiration from the highly successful open-source project DataScript\cite{datascript}, we implement relations as hashmaps, with generic hashing
functions. The point of using a hashmap is that it allows for one to easily disregard duplicates altogether, further simplifying the implementation.

Indexes on the other hand, are a vital optimization aspect. A considerable amount of recent datalog research has dealt with speeding up relational joins
with respect to datalog, such as in attempting to specialize data structures to it\cite{souffle_btree}, and rminimizing the number of necessary
joins to evaluate a $SPJ$ expression\cite{primitive_search}. Thus, a trait \verb|IndexBacking| is defined, whose requirements are two methods, one
that takes in a row for insertion, and another that requires a join implementation.

We implement the \verb|IndexBacking| trait for implementations of all most used data structures for datalog indexes, such as the Rust standard
library BTree\cite{rust_btree} and HashMap, persistent implementations of both them, a regular vector, that naively sorts itself before a
join, and the Spine, an experimental data structure, that empirically shows good performance characteristics in datalog workflows.

\subsection{The Spine}
The Spine is a simple two-level pointer-free data structure comprised of an array of arrays, and an index. Sorted Arrays are the fastest data structure for
binary search lookups. Compared to search trees, they are much more cache efficient, since every single piece of data is in one contiguous region
in memory, and no pointer indirection is needed in every binary search iteration.

Insertion in sorted arrays is commonly implemented throgh an emulated binary insertion sort, where the position of the to-be-inserted element
is first found through a binary search, and then all elements to the right of its supposed position are shifted. The complexity of this
operation is $O(n)$, therefore being exponentially slower than search trees, rendering it unusuable in dynamic cases, such as in datalog
evaluation, in which possibly costly bulk insertions occur at a very fast pace.

In modern CPU architectures, there are multiple levels of memory, going from CPU registers, L-caches, up to disk. Accessing one element
cached in low-level memory can be hundreds of thousands of times faster than doing so in the disk, therefore it is possible, in practice, for
asymptotic linear-time access to be faster than logarithmic.

The Spine is a naive attempt to take advantage of that, by keeping a totally ordered set of fixed-capacity $B$ sorted arrays, ordered by their
maximum, with $B$ being determined empirically to be the value such that the performance ratio of insertion and search is as desired. Moreover, given
that the most efficient data structure for datalog indexing, BTree, also relies on sorted data, it often occurs that operations benefit from both
spatial and temporal cache locality.

Souffle's datalog-tailored B-Tree\cite{souffle_btree} has a simple yet effective \textit{hint} mechanism for speeding up searches and insertions, by
taking advantage of locality. Whenever a new row is inserted, its traversal is kept as a \textit{hint}, therefore if the next row to be inserted
falls within the same \textit{range} as the previous, the \textit{range} itself can be searched, possibly entirely avoiding a traversal, and only
searching in the child node. This same mechanism is implemented in the Spine, with the help of its index.

Fenwick Trees\cite{fenwick_tree} are highly specialized data structures used for dynamic cumulative sum. Given a fixed-length array of counts, the
fenwick tree allows for calculating the prefix sum in $O(\log_2{n})$ time, while also providing adjustments to counts in the same bounds. We
leverage the \cite{fenwick_tree} in order to maintain an index that will keep track of the size of each internal array. The cost of updating
the fenwick tree is minimal, in the context of the Spine, taking $O(\log_2(B))$ time with $B \ll n$.

In order to ensure that all sorted sub-arrays have at most $B$ elements, a \textit{maintenance} operation must occur. Whenever a sub-array \textit{overflows},
it is split in half, with all arrays to the right of the overflown one shifted one place, and a new array being created, moving all elements that belonged
to the upper half. The cost of this operation, alongside the respawning of the Fenwick Tree, is $O(B)$, which is, in practice, negligible to most
workloads.

The Spine leverages the Fenwick Tree in order to support $O(\log_2(B))$-time accesses by position. In order to locate the sorted array in which the element
in the global $i$-th position among the union of all sorted arrays is, naively, one can run a binary search over the fenwick tree, calling prefix sum on
each iteration. Given that, as previously mentioned, prefix sum takes $O(\log_2(N))$ time, a binary search that calls prefix sum on each iteration would
take $O(\log^2_2(N))$. It is possible to leverage the structure of the fenwick tree, and calculate that, in a very efficient manner, taking only $O(\log_2{N})$ time.

The algorithm to do so is as follows:

\begin{algorithm} [!h]
	\scriptsize
	\KwIn{a fenwick tree $\mathcal{F}$, a desired position $i$-th}
	\KwResult{the count inside $F$ in which $i$-th would be in}
	\SetKwData{s}{s}
	\BlankLine
	\SetKwProg{Fn}{Function}{ is}{end}
	\SetKwProg{While}{While}{:}{end}

	let length = $\mathcal{F}$.length()

	let prefix\_sum = $i$

	let mut idx = 0;

	let mut $x$ = most significant bit of length * 2

	\While {$x \ge 0$}{
		let lsb = least significant bit of x

		\If{$x \leq$ length, $\mathcal{F}[x - 1] \le$ prefix\_sum} {
			idx = x

			prefix\_sum = prefix\_sum - $\mathcal{F}[x - 1]$

			x = x + lsb / 2;
		}\Else{
			\If {lsb is even} {
				break
			}
			x = lsb / 2 - lsb
		}
	}


	\textbf{return} idx
	\caption{An algorithm to binary search a fenwick tree in logarithmic time}
	\label{alg:fen_tree}
\end{algorithm}

Being logarithmic with respect to $B$ access time, we can have an improved version of hints. In the original proposal, the cost of hint is to retain a pointer
to the leaf node that it is in. Let there be a search, with a query point $k$ and a hint $h$. When a search starts, in case the element isn't in the leaf
child of the hint pointer, then a traversal starts from zero, this implies the worst-case cost will be $O(\log(N))$. On the Spine in the other hand, first $k$
is compared with $h$, and then a binary search is started. if $k > h$, then $h$ is the lower bound, if else, then $k$ is the upper bound. This provides
significantly more value than the simple child lookup in souffle's B-Tree.

% Fenwick Tree Binary Search 

\subsection{Differential Dataflow}

The \verb|Differential| engine is an attempt at implementing a substitution-based engine on top of the distribution framework that addresses datalog's
evaluation inefficiencies in a manner that no other currently available framework does, differential dataflow\cite{dd}. Most of the distributed datalog engines are
built on top of either graph-processing or map-reduce frameworks, both kinds of projects that were not specifically made with expressive query answering in mind, but
more with efficiently handling complex non-recursive queries over large amounts of data.

One of the biggest challenges in efficiently evaluating datalog is in the deletion of data. Incremental bottom-up processing of additions is already the
manner in which semi-naive evaluation works, and is relatively efficient. The other direction is significantly more complex. In order to delete a fact,
one has to take into account that it might have influenced the inferrence of other facts, which imply having to look over the whole data, while the opposite
direction does not require that, therefore incurring possibly very large performance differences between both directions.

Similarly to how semi-naive evaluation is the \textit{de facto} method for incremental bottom-up evaluation, the Delete-Rederive\cite{dred} algorithm holds
the same regard with respect to the incremental maintenance of bottom-up evaluation with respect to deletions. There are two stages to it, the first, Deletion,
naively overdeletes all facts that could have been derived from it, but at the same time, keeps track of possible facts that could have had alternative
derivations. The second stage, rederive, restarts the evaluation process.

Differential dataflow directly addresses this issue by evaluating both additions and deletions in the same manner, while at the same time efficiently parallelizing
semi-naive evaluation, however, it is first necessary to digress over Timely Dataflow~\cite{timely}, the underlying networked computation system in which differential
is built upon.

Timely is a high-throughput, low-latency streaming-and-distributed model that extends dataflow. The timely in its name refers to it thoroughly establishing
the semantics of each aspect of communication notification, taking the parallelization blueprint and making its execution transparent to the user. A notification
sent between operators is assigned a timestamp, that needs only to be partially ordered, with no assumptions being made with respect to insertion. The goal is to use
them to correctly reason about the data with respect to the local operator states' of computation completion, in order to cleverly know with certainty when not only
whether should work be done asynchronously or with sharding, but whether there is data yet to be processed or not.

Differential Dataflow, a generalization of incremental processing, is an extension to the Timely Dataflow model. While the shape of the data that flows in the latter is
in the form of \verb|(data, timestamp)|, the former's is \verb|(data, timestamp, multiplicity)|, therefore restricting its bag-of-data semantics to multisets, with
the multiplicity element representing a difference, for instance, $+1$ representing the addition of $1$ datapoint arriving at a specific timestamp, or $-1$, a deletion. The
key difference between both models is that similarly to how timely makes distribution transparent, differential does the same with incremental computation, by providing to the
user the possibility of not only adding to a stream, the only possible action in timely dataflow, but allow it to respond to arbitrary additions and retractions at any timestamp.

Similarly to how timestamps in timely are the key element to the efficient parallelization of iterative dataflows, they can also be used in incremental scenarios in order to
overcome their inherently sequential nature. This is of paramount importance to the performance of datalog evaluation.

Let's assume that there is some dataflow that computes the transitive closure of some graph $d_0$, and one update consisted of four edge differences, labeled as $\delta$, arrives, with
the resulting updated graph being $d_1$. In a regular incremental system, each triple would increment the iteration counter by $1$, and even though the relational operations inside
the dataflow might happen in parallel, $\delta_1$, $\delta_2$, $\delta_3$ and $\delta_4$ will only be evaluated after $\delta_0$ and each of its sequent difference's iteration finishes.

If that dataflow were to be executed with differential dataflow, inside the iteration scope there would be a product timestamp $\langle a, b \rangle$, with $a$ representing the time
in which some initial triple was fed into the computation, and $b$ denoting the iteration count, therefore tracking the transitive chain length, respecting the
following partial order: \[\langle a_i, b_1 \rangle \leq \langle a_j, b_j \rangle \iff a_i \leq a_j \wedge b_i \leq b_j\]

If we take that $\delta_0$, $\delta_3$, $\delta_1$ and $\delta_2$ are differences with the following respective timestamps: $\langle 0, 1 \rangle, \langle 0, 2 \rangle, \langle 1, 1 \rangle, \langle 1, 2 \rangle$ it
is noticiable that $\delta_0$ and $\delta_3$ are comparable, but both of them, and vice-versa, are incomparable with respect to $\delta_1$ and $\delta_2$. This, in turn, means that, as it can
these pairs of differences, despite notifying change on the same iteration operator, could safely be executed in parallel in differential dataflow, but not in a regular
incremental system that uses totally-ordered timestamps.

The most notable difference between iterative and incremental processing is that in the latter computation advances by, ideally, making adjustments proportional to the newly
received data, referred to as difference. Naturally, this could result in massively reduced latency, however, in order to support this in the first place, incremental
systems have to maintain indexes of all updates, be it an addition or a retraction, that could impact the calculation.

The differential dataflow implementation, built on top of timely dataflow's rust one, utilizes a novel method to maintain indexes such that they are not restricted
to each individual operator, and could also be shared between multiple readers. This method utilizes shared arrangements, out of which its core concept, collection
trace, is a structure that represents the state of a collection since the latest frontier, as explained in the timely dataflow section, as an append-only ordered sequence
of batches of updates, with each batch possibly being merged with other batches, that make up an LSM-Tree~\cite{lsm}, therefore benefitting from its compaction mechanism
to ensure that only a logarithmic number of batches exist.

\section{Experiments}

I actually did some of the experiments. results were promising, trust me :D this will revolutise the field!

\bibliographystyle{ACM-Reference-Format}
\bibliography{software}

\end{document}
\endinput